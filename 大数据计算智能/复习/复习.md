<h1>大数据计算智能复习</h1>

[toc]

## 组件化思想

### 模型或模式结构

- 模型：
  - 对整个数据的高层次、全局化的描述（全局模型）
  - 如线性回归模型、层次聚类模型
- 模式：
  - 对一小部分数据进行描述，是局部的（局部模式）
  - 如频繁序列模式

### 数据挖掘任务

- 模型挖掘
  - 预测建模（有监督）
  - 描述建模（无监督）
- 模式挖掘
  - 频繁模式
  - 异常模式

### 评分函数

- 似然函数
- 误差平方和
- MSE
- MSRE
- 准确率

### 搜索和优化方法

- 常用的优化方法
  - 爬山（Hill-Climbing）
  - 最陡峭下降
  - 期望最大化（Em）
- 常用的搜索方法
  - 贪婪搜索
  - 分支界定
  - DFS/BFS

### 数据管理策略
- 没什么重要的

## 频繁模式
### Apriori
- 关联规则
  - 长这样：$A\rightarrow B$、$A\cap B\rightarrow C$
  - 支持度
    - 所有的项中，左右两边的目标项**共同出现**的概率
  - 可信度
    - 所有的项中，右边的，在左边的出现的项里的概率。（条件概率）
  - 频繁项集的定义：出现频度大于最小支持度
- 重要公理
  - 如果项目集$S$是频繁的，那么$S$的任意子集也是频繁的。
  - 它的逆否命题是Apriori算法的关键。
- 算法步骤
  - 分层寻找频繁项目集
    - 一个$C$，一个$L$
    - $C$中数，$L$中筛
    - 结束条件，$L$是空集了
  - 产生关联规则
    - 取频繁项集$L$的所有非空子集$S$
    - 如果$\cfrac{support(L)}{support(S)}\ge min\_confidence$，那么就输出关联规则$S\rightarrow(L-S)$
### GSP
- 序列模式挖掘的算法，过程跟Apriori几乎一样，只是多了一点预处理
- 与Apriori的区别
  - 序列模式生成候选集时，需要关注顺序，即12与21不同
  - 如果相同的项目集合在同一个序列里出现多次的话，只算一次
- 算法步骤
  - 先生成1-项集，然后扫描所有序列来筛选1-项集，筛选完后的1-项集连接成2-项集。重复直至项集都生成完。
  - 将项集映射成1，2，3这些数字，然后生成转换后的频繁项集
  - 执行Apriori
### 相关系数
- $Corr_{A,B}=\cfrac{P(A\cup B)}{P(A)P(B)}$
  - 公式还可以进一步转换，但好像没必要。$\cfrac{P(B|A)}{P(B)}$
- $Corr_{A,B}<1$，负相关，互相排斥
- $Corr_{A,B}=1$，不相关，互相独立
- $Corr_{A,B}<1$，正相关，互相吸引
### FP Growth
- 只要会给数据集，构造生成FP树就行
- 看PPT-e的第九页

## 分类算法
### 决策树
- 决策树越矮越好，每次分的时候选择收益最大的那个属性。收益可以是很多标准，如信息增益、信息增益比
- 信息熵
  - $-\sum_{i}p_i\log(p_i)$
  - 因为都是概率，所以$p$都是小于1的，因此取对数之后就是负值，要再加个负号让它变回正的。
- 信息增益
  - 信息增益就是前后信息熵的差
  - $Entory(S)$表示未选择任何特征列时的信息熵。$Entory(A,S)$表示选择了特征列$A$之后的信息熵
  - 注意一下PPT-a第42页的符号表示，考试的时候这样写会快一点
- 信息增益比
  - $GainRatio=\cfrac{Gain(A)}{SplitInfo(A)}$
  - $SpiltInfo(A)$就是以特征列$A$为分类标准算的信息熵
- 剪枝
  - 总结：后剪比较好
  
  ||时间开销|过/欠拟合风险|
  |:-:|:-:|:-:|
  |先剪|训练时间开销**降低**，测试时间开销**降低**|过拟合风险**降低**，欠拟合风险**增加**|
  |后剪|训练时间开销**增加**，测试时间开销**降低**|过拟合风险**降低**，欠拟合风险**基本不变**|
### 贝叶斯
- 给定一组特征$X$，按照朴素的思想将$P(D|X)=\cfrac{P(X|D)P(D)}{P(X)}=\cfrac{P(x_1|D)P(x_2|D)\cdots P(x_n|D)P(D)}{P(X)}$特征之间独立
- 朴素是因为假设了
- 在计算的时候可以不管分母，因为大家分母都一样，因此没必要算
- 拉普拉斯修正
  - 因为有时候会出现某个条件概率为0的情况
  - 因此给大家都加个1
  - 参考PPT-c第32页到第34页

### 神经网络
- 要知道卷积怎么算
- 各类梯度下降的区别
  - 批量梯度下降法（BGD）
    - 用所有的数据作梯度下降
  - 随机梯度下降法（SGD）
    - 只挑选一个数据作梯度下降
  - 小批量梯度下降法
    - 上面两者的折中
### KNN
- 要会算KNN
- 评价方式
  |机器↓真实结果→|True|False|
  |:-:|:-:|:-:|
  |True|a|b|
  |False|c|d|
  - $P(precision)$ 准确率
    - $\cfrac{a}{a+b}$
  - $R(Recall)$ 召回率
    - $\cfrac{a}{a+c}$
  - F值
    - $\cfrac{1}{\alpha P+(1-\alpha)R}$ 
    - $\alpha$在0到1之间
    - 特殊情况$F_1=\cfrac{2PR}{P+R}$

## 聚类算法
### 基于划分（kmeans）
- 要知道kmeans怎么迭代
- 要会算距离，L1距离，L2距离
- 优点
  - 简单
  - 快速
  - 并且能够适应于大数据集
- 缺点
  - 受到初始点的影响
  - 对数据集的分布形状要求比较严格
  - 对离群值很敏感
  - 对噪音很敏感
  - 需要提前设定$k$的大小
- 使用的搜索和优化方法是爬山（Hill-Climbing）

### 基于密度（DBSCAN）
- 直接密度可达（DDR）
  - 一个点在另一个点的圈内
- 密度可达（DR）
  - 一个点和另一个点不是直接密度可达，但两者之间，可以通过直接密度可达的传递性连接起来
- 密度可连接（DC）
  - 一个点和另一个点不是密度可达，但两者之间可以通过密度可达连接起来。
- 优点
  - 对任意形状都有较好的适应性
  - 对噪声具有鲁棒性
  - 不需要提前设定$k$的大小
- 缺点
  - 很难提前设定好$\epsilon$和$MinPts$的大小
  - 数据集的密度分布是不确定的
  - 受到“chaining effect”的影响很大
- 改进算法OPTICS
  - 解决了DBSCAN对输入参数$\epsilon$和$MinPts$的敏感性问题
  - 可以合并层次关系
### 基于层次
- 基于聚合的层次聚类
  - 如 AGNES
- 基于划分的层次聚类
  - 如 DIANA
- 4个距离
  - $d_{max}$：两个族的最大距离
  - $d_{min}$：两个族的最小距离
  - $d_{mean}$：两个族的中心的距离
  - $d_{avg}$：两个族中两两计算的平均距离
- CF(N, LS, SS)
  - N：数据点的数量
  - LS：$\sum X$
  - SS：$\sum X^2$
### 基于模型


## 推荐系统
- 四种方法
  - 基于内容推荐
  - 基于协同过滤推荐
  - 基于知识推荐
  - 混合推荐